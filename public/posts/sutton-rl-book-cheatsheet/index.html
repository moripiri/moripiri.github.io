<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Sutton의 RL 책 한 장 요약 | mori-blog</title>
<meta name="keywords" content="RL">
<meta name="description" content="한 페이지로 보는 Sutton의 RL 책">
<meta name="author" content="mori">
<link rel="canonical" href="https://moripiri.github.io/posts/sutton-rl-book-cheatsheet/">
<meta name="google-site-verification" content="googleccca69acea2b3187">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://moripiri.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://moripiri.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://moripiri.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://moripiri.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://moripiri.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://moripiri.github.io/posts/sutton-rl-book-cheatsheet/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
    ]
  });"></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-18KKC7TB58"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-18KKC7TB58');
        }
      </script><meta property="og:url" content="https://moripiri.github.io/posts/sutton-rl-book-cheatsheet/">
  <meta property="og:site_name" content="mori-blog">
  <meta property="og:title" content="Sutton의 RL 책 한 장 요약">
  <meta property="og:description" content="한 페이지로 보는 Sutton의 RL 책">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-15T11:30:03+00:00">
    <meta property="article:modified_time" content="2025-06-15T11:30:03+00:00">
    <meta property="article:tag" content="RL">
    <meta property="og:image" content="https://moripiri.github.io/%3Cimage%20path/url%3E">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://moripiri.github.io/%3Cimage%20path/url%3E">
<meta name="twitter:title" content="Sutton의 RL 책 한 장 요약">
<meta name="twitter:description" content="한 페이지로 보는 Sutton의 RL 책">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://moripiri.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sutton의 RL 책 한 장 요약",
      "item": "https://moripiri.github.io/posts/sutton-rl-book-cheatsheet/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Sutton의 RL 책 한 장 요약",
  "name": "Sutton의 RL 책 한 장 요약",
  "description": "한 페이지로 보는 Sutton의 RL 책",
  "keywords": [
    "RL"
  ],
  "articleBody": "Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew Barto: link\nIntroduction Reinforcement Learning: An area of machine learning that aims to learn what to do to maximize cumulative reward.\nAgents and Environments At each timestep $t$:\nAgents: Receive Reward $R_t$ and Observation $O_t$, execute Action $A_t$ Environments: Receive Action $A_t$, then emit Reward $R_{t+1}$ and Next Observation $O_{t+1}$ The sequence of observations, actions, and rewards is called history.\n$$\\begin{aligned} H_t = O_1, R_1, A_1, … , A_{t-1}, O_t, R_t \\end{aligned}$$ State$(S_t)$: information used to determine what happens next. Both the agent and environment have state, and they may not agree with each other.\nFully observable environments: $O_t = S_t$. The agent can know the exact state of the environment. It is called a Markov Decision Process (MDP) Partially observable environments: $O_t \\neq S_t$. The agent only knows the partial state of the environment. It is called a Partially Observable Markov Decision Process (POMDP) Markov Decision Processes (MDP) Markov Decision Process (MDP) is an environment that can be defined as a 5-tuple:\n$$\\begin{aligned} (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma) \\end{aligned}$$\nwhere\n$\\mathcal{S}$: state space (set of states)\n$\\mathcal{A}$: action space (set of actions)\n$\\mathcal{P}$: state transition probability matrix $\\mathcal{P}=\\mathbb{P}[S_{t+1}=s’|S_t=s, A_t=a]$\n$\\mathcal{R}$: reward function $\\mathcal{R} = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$\n$\\gamma$: discount factor $\\gamma \\in [0, 1]$\nMarkov Property $$\\begin{aligned} \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, S_2, … , S_t] \\end{aligned}$$\nPolicy $$\\begin{aligned} \\pi(a|s) = \\mathbb{P}[A_{t} = a|S_t = s] \\end{aligned}$$\nReturn $$\\begin{aligned} G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\end{aligned}$$\nwhere $\\gamma \\in [0, 1]$\nValue function \u0026 Action-value function (Q-value function) $$\\begin{aligned} v_\\pi(s) = \\mathbb{E_\\pi}[G_t|S_t = s] \\end{aligned}$$\n$$\\begin{aligned} q_\\pi(s, a) = \\mathbb{E_\\pi}[G_t|S_t = s, A_t = a] \\end{aligned}$$\nBellman equation Let’s derive the Bellman expectation equation for $v_\\pi(s)$.\n$$ \\begin{aligned} v_{\\pi}(s) \u0026= \\mathbb{E_\\pi}[G_t \\mid S_t = s] \\\\ \u0026= \\mathbb{E_\\pi} \\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\mid S_t = s \\right] \\\\ \u0026= \\mathbb{E_\\pi} \\left[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\mid S_t = s \\right] \\\\ \u0026= \\mathbb{E_\\pi} \\left[R_{t+1} + \\gamma v_\\pi(s_{t+1}) \\mid S_t = s \\right] \\end{aligned} $$\n$$ \\begin{aligned} q_\\pi(s, a) \u0026= \\mathbb{E_\\pi}[R_{t+1} + \\gamma q_\\pi (S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a] \\end{aligned} $$\nOptimal value functions $$\\begin{aligned} \\pi \\geq \\pi’ \\text{ iff } v_\\pi (s) \\geq v_{\\pi’} (s) \\text{ for } \\forall s \\in \\mathcal{S} \\end{aligned}$$\n$$\\begin{aligned} v_*(s) = \\max_{\\pi} v_\\pi (s) \\end{aligned}$$\n$$\\begin{aligned} q_*(s, a) = \\max_{\\pi} q_\\pi (s, a) \\end{aligned}$$\nfor all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$.\n$$\\begin{aligned} v_\\ast(s) = \\max_{a} q_\\ast (s, a) \\end{aligned}$$\n$$\\begin{aligned} v_\\ast(s) = \\mathbb{E_\\pi}[R_{t+1} + \\gamma v_\\ast (S_{t+1})|S_t = s] \\end{aligned}$$\n$$\\begin{aligned} q_*(s, a) = \\mathbb{E_\\pi}[R_{t+1} + \\gamma \\max_{a’} q_\\ast (S_{t+1}, a’)|S_t = s, A_t = a] \\end{aligned}$$\nDynamic Programming What is Dynamic Programming? Dynamic programming (DP): a collection of algorithms that can be used to compute optimal policies given a perfect model such as a Markov Decision Process (MDP).\nDP can solve problems that have two properties:\nOverlapping subproblems Problem can be broken down into subproblems Solutions to subproblems can be reused Optimal substructure Problem can be decomposed into subproblems Policy Iteration Value Iteration Model-Free Prediction Model-free prediction algorithms aim to estimate the value function of a certain policy without knowing the MDP.\nMonte-Carlo Prediction Temporal-Difference Learning Difference between Monte-Carlo (MC) and Temporal-Difference (TD) Model-Free Control Model-free control algorithms can be divided into two groups: on-policy control and off-policy control.\nOn-policy control: Learn about policy $\\pi$ from experience sampled from $\\pi$\nOff-policy control: Learn about policy $\\pi$ from experience sampled from policy $\\mu \\neq \\pi$\nBoth types of control algorithms are widely utilized in RL.\nOn-policy Monte-Carlo Control On-policy Temporal-Difference Control Off-policy Temporal-Difference Control Policy Gradient Methods In policy gradient, policy parameter $\\theta$ is updated by some scalar performance measure $\\mathcal{J}(\\theta)$ with respect to the policy parameter. To maximize policy performance, their updates approximate gradient ascent in $\\mathcal{J}$:\n$$\\begin{align} \\theta_{t+1} = \\theta_t + \\alpha \\nabla \\mathcal{J}(\\theta_t) \\end{align}$$\nwhere $\\alpha$ is a step-size parameter.\nThen what can be the performance measure $\\mathcal{J}(\\theta)$ for an MDP policy in finite episodes?\nIn the episodic case, we have trajectory $\\tau$. Then, we can define $\\mathcal{J}(\\theta)$ as its value.\n$$\\begin{align} \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s)v_\\pi(s) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) q_\\pi(s, a) \\end{align}$$\nwhere $d^\\pi$ is the stationary distribution for the Markov chain for $\\pi_\\theta$.\nThen the gradient of $\\mathcal{J}$ can be reformatted as the following:\n$$\\begin{aligned} \\nabla_\\theta \\mathcal{J}(\\theta) \u0026= \\nabla_\\theta \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) Q_\\pi(s, a) \u0026 \\\\ \u0026\\propto \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} Q_\\pi(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s, a) Q_\\pi(s, a) \\dfrac{\\nabla_\\theta \\pi_\\theta(a|s)}{\\pi_\\theta(a|s)} \\\\ \u0026= \\mathbb{E_\\pi} [Q_\\pi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)] \\end{aligned}$$\nwhere $\\mathbb{E_\\pi}$ refers to $\\mathbb{E_{s \\sim d_\\pi, a \\sim \\pi_\\theta}}$ and $Q_\\pi$ means the true state-value under policy $\\pi$.\nPolicy Based Reinforcement Learning RL algorithms that directly generate the policy from experience: Policy-based RL.\nMonte-Carlo Policy Gradient $$\\begin{align} \\mathbb{E_\\pi} [G_t|s_t, a_t] = Q_\\pi (s_t, a_t) \\end{align}$$\nActor-Critic Policy Gradient Yet the algorithm REINFORCE has a disadvantage of high gradient variance.\nThus, to reduce variance, a critic can be used instead of return to estimate $Q_\\pi$.\n$$\\begin{align} q_\\phi (s, a) = Q_\\pi(s, a) \\end{align}$$\nTherefore, the policy gradient is changed as\n$$\\begin{align} \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E_\\pi} [q_\\phi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)]\\ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) q_\\phi(s_t, a_t) \\end{align}$$\nCritic can be updated by\nMonte-Carlo evaluation TD(0) TD($\\lambda$) For example, if we use TD(0), then in timestep $t$ the critic will be updated as\n$$\\begin{align} \\phi \\leftarrow \\phi + \\beta (r_t + \\gamma q_\\phi(s_{t+1}, a_{t+1}) - q_\\phi(s_t, a_t)) \\nabla_\\phi q_\\phi(s_t, a_t) \\end{align}$$\nModel-based Reinforcement Learning Model-based RL: it learns a model from experience and plans value function and/or policy from the model.\nWhat is a Model? A Model $\\mathcal{M}$ is a representation of an MDP \u003c$\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}$\u003e parametrized by $\\eta$. Assuming we know the state space $\\mathcal{S}$ and action space $\\mathcal{A}$, a model $\u003c\\mathcal{P}, \\mathcal{R}\u003e$ can represent transitions\n$$\\begin{align} S_{t+1} \\sim \\mathcal{P_\\eta(S_{t+1}|S_t, A_t)} \\\\ R_{t+1} \\sim \\mathcal{R_\\eta(R_{t+1}|S_t, A_t)} \\end{align}$$\nThe model $\u003c\\mathcal{P}, \\mathcal{R}\u003e$ is learned from experience ${S_1, A_1, R_2, … , S_T}$.\nA model can be parametrized in various ways: from lookup table models to deep neural networks.\n",
  "wordCount" : "1028",
  "inLanguage": "en",
  "image":"https://moripiri.github.io/%3Cimage%20path/url%3E","datePublished": "2025-06-15T11:30:03Z",
  "dateModified": "2025-06-15T11:30:03Z",
  "author":{
    "@type": "Person",
    "name": "mori"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://moripiri.github.io/posts/sutton-rl-book-cheatsheet/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "mori-blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://moripiri.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://moripiri.github.io/" accesskey="h" title="Mori&#39;s blog (Alt + H)">
                <img src="https://moripiri.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Mori&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://moripiri.github.io/posts/" title="posts">
                    <span>posts</span>
                </a>
            </li>
            <li>
                <a href="https://moripiri.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="https://moripiri.github.io/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js" integrity="sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js" integrity="sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<script type="module">
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://moripiri.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://moripiri.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Sutton의 RL 책 한 장 요약
    </h1>
    <div class="post-description">
      한 페이지로 보는 Sutton의 RL 책
    </div>
    <div class="post-meta"><span title='2025-06-15 11:30:03 +0000 +0000'>June 15, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1028 words&nbsp;·&nbsp;mori&nbsp;|&nbsp;<a href="https://github.com/moripiri/moripiri.github.io/issues/posts/Sutton-RL-book-cheatsheet/index.md" rel="noopener noreferrer edit" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#agents-and-environments">Agents and Environments</a></li>
        <li><a href="#markov-decision-processes-mdp">Markov Decision Processes (MDP)</a></li>
      </ul>
    </li>
    <li><a href="#dynamic-programming">Dynamic Programming</a>
      <ul>
        <li><a href="#what-is-dynamic-programming">What is Dynamic Programming?</a></li>
        <li><a href="#policy-iteration">Policy Iteration</a></li>
        <li><a href="#value-iteration">Value Iteration</a></li>
      </ul>
    </li>
    <li><a href="#model-free-prediction">Model-Free Prediction</a>
      <ul>
        <li><a href="#monte-carlo-prediction">Monte-Carlo Prediction</a></li>
        <li><a href="#temporal-difference-learning">Temporal-Difference Learning</a></li>
        <li><a href="#difference-between-monte-carlo-mc-and-temporal-difference-td">Difference between Monte-Carlo (MC) and Temporal-Difference (TD)</a></li>
      </ul>
    </li>
    <li><a href="#model-free-control">Model-Free Control</a>
      <ul>
        <li><a href="#on-policy-monte-carlo-control">On-policy Monte-Carlo Control</a></li>
        <li><a href="#on-policy-temporal-difference-control">On-policy Temporal-Difference Control</a></li>
        <li><a href="#off-policy-temporal-difference-control">Off-policy Temporal-Difference Control</a></li>
      </ul>
    </li>
    <li><a href="#policy-gradient-methods">Policy Gradient Methods</a>
      <ul>
        <li><a href="#policy-based-reinforcement-learning">Policy Based Reinforcement Learning</a></li>
        <li><a href="#monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</a></li>
        <li><a href="#actor-critic-policy-gradient">Actor-Critic Policy Gradient</a></li>
      </ul>
    </li>
    <li><a href="#model-based-reinforcement-learning">Model-based Reinforcement Learning</a>
      <ul>
        <li><a href="#what-is-a-model">What is a Model?</a></li>
      </ul>
    </li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Reinforcement Learning: An Introduction</strong> by Richard S. Sutton and Andrew Barto: <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">link</a></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Reinforcement Learning: An area of machine learning that aims to learn what to do to maximize cumulative reward.</p>
<h3 id="agents-and-environments">Agents and Environments<a hidden class="anchor" aria-hidden="true" href="#agents-and-environments">#</a></h3>
<p align="center">
 <img src = "images/agent_environment.png">
</p>
<p>At each timestep $t$:</p>
<ul>
<li><strong>Agents</strong>: Receive Reward $R_t$ and Observation $O_t$, execute Action $A_t$</li>
<li><strong>Environments</strong>: Receive Action $A_t$, then emit Reward $R_{t+1}$ and Next Observation $O_{t+1}$</li>
</ul>
<p>The sequence of observations, actions, and rewards is called history.</br></p>
<p>$$\begin{aligned}
H_t = O_1, R_1, A_1, &hellip; , A_{t-1}, O_t, R_t
\end{aligned}$$
</br>
</br>
<strong>State</strong>$(S_t)$: information used to determine what happens next. Both the agent and environment have state, and they may not agree with each other.</p>
<ul>
<li><strong>Fully observable environments</strong>: $O_t = S_t$. The agent can know the exact state of the environment. It is called a <strong>Markov Decision Process (MDP)</strong></li>
<li><strong>Partially observable environments</strong>: $O_t \neq S_t$. The agent only knows the partial state of the environment. It is called a <strong>Partially Observable Markov Decision Process (POMDP)</strong></li>
</ul>
<h3 id="markov-decision-processes-mdp">Markov Decision Processes (MDP)<a hidden class="anchor" aria-hidden="true" href="#markov-decision-processes-mdp">#</a></h3>
<p><strong>Markov Decision Process (MDP)</strong> is an environment that can be defined as a 5-tuple:</p>
<p>$$\begin{aligned}
(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)
\end{aligned}$$</p>
<p>where</p>
<ul>
<li>
<p>$\mathcal{S}$: state space (set of states)</p>
</li>
<li>
<p>$\mathcal{A}$: action space (set of actions)</p>
</li>
<li>
<p>$\mathcal{P}$: state transition probability matrix $\mathcal{P}=\mathbb{P}[S_{t+1}=s&rsquo;|S_t=s, A_t=a]$</p>
</li>
<li>
<p>$\mathcal{R}$: reward function $\mathcal{R} = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$</p>
</li>
<li>
<p>$\gamma$: discount factor $\gamma \in [0, 1]$</p>
</li>
</ul>
<h4 id="markov-property">Markov Property<a hidden class="anchor" aria-hidden="true" href="#markov-property">#</a></h4>
<p>$$\begin{aligned}
\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, S_2, &hellip; , S_t]
\end{aligned}$$</p>
<h4 id="policy">Policy<a hidden class="anchor" aria-hidden="true" href="#policy">#</a></h4>
<p>$$\begin{aligned}
\pi(a|s) = \mathbb{P}[A_{t} = a|S_t = s]
\end{aligned}$$</p>
<h4 id="return">Return<a hidden class="anchor" aria-hidden="true" href="#return">#</a></h4>
<p>$$\begin{aligned}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \cdots  = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{aligned}$$</p>
<p>where $\gamma \in [0, 1]$</p>
<h4 id="value-function--action-value-function-q-value-function">Value function &amp; Action-value function (Q-value function)<a hidden class="anchor" aria-hidden="true" href="#value-function--action-value-function-q-value-function">#</a></h4>
<p>$$\begin{aligned}
v_\pi(s) = \mathbb{E_\pi}[G_t|S_t = s]
\end{aligned}$$</p>
<p>$$\begin{aligned}
q_\pi(s, a) = \mathbb{E_\pi}[G_t|S_t = s, A_t = a]
\end{aligned}$$</p>
<h4 id="bellman-equation">Bellman equation<a hidden class="anchor" aria-hidden="true" href="#bellman-equation">#</a></h4>
<p>Let&rsquo;s derive the Bellman expectation equation for $v_\pi(s)$.</p>
<p>$$
\begin{aligned}
v_{\pi}(s) &amp;= \mathbb{E_\pi}[G_t \mid S_t = s]  \\
&amp;= \mathbb{E_\pi} \left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s \right]  \\
&amp;= \mathbb{E_\pi} \left[R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k R_{t+k+2} \mid S_t = s \right]  \\
&amp;= \mathbb{E_\pi} \left[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t = s \right]
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
q_\pi(s, a) &amp;= \mathbb{E_\pi}[R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a]
\end{aligned}
$$</p>
<h4 id="optimal-value-functions">Optimal value functions<a hidden class="anchor" aria-hidden="true" href="#optimal-value-functions">#</a></h4>
<p>$$\begin{aligned}
\pi \geq \pi&rsquo; \text{  iff  } v_\pi (s) \geq v_{\pi&rsquo;} (s) \text{  for  } \forall s \in \mathcal{S}
\end{aligned}$$</p>
<p>$$\begin{aligned}
v_*(s) = \max_{\pi} v_\pi (s)
\end{aligned}$$</p>
<p>$$\begin{aligned}
q_*(s, a) = \max_{\pi} q_\pi (s, a)
\end{aligned}$$</p>
<p>for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$.</p>
<p>$$\begin{aligned}
v_\ast(s) = \max_{a} q_\ast (s, a)
\end{aligned}$$</p>
<p>$$\begin{aligned}
v_\ast(s) = \mathbb{E_\pi}[R_{t+1} + \gamma v_\ast (S_{t+1})|S_t = s]
\end{aligned}$$</p>
<p>$$\begin{aligned}
q_*(s, a) = \mathbb{E_\pi}[R_{t+1} + \gamma \max_{a&rsquo;} q_\ast (S_{t+1}, a&rsquo;)|S_t = s, A_t = a]
\end{aligned}$$</p>
<h2 id="dynamic-programming">Dynamic Programming<a hidden class="anchor" aria-hidden="true" href="#dynamic-programming">#</a></h2>
<h3 id="what-is-dynamic-programming">What is Dynamic Programming?<a hidden class="anchor" aria-hidden="true" href="#what-is-dynamic-programming">#</a></h3>
<p><strong>Dynamic programming (DP)</strong>: a collection of algorithms that can be used to compute optimal <strong>policies</strong> given a perfect model such as a <strong>Markov Decision Process (MDP).</strong></p>
<p>DP can solve problems that have two properties:</p>
<ol>
<li><strong>Overlapping subproblems</strong>
<ul>
<li>Problem can be broken down into subproblems</li>
<li>Solutions to subproblems can be reused</li>
</ul>
</li>
<li><strong>Optimal substructure</strong>
<ul>
<li>Problem can be decomposed into subproblems</li>
</ul>
</li>
</ol>
<h3 id="policy-iteration">Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#policy-iteration">#</a></h3>
<p align="center">
 <img src = "images/policy_iteration_pseudocode.png">
</p>
<h3 id="value-iteration">Value Iteration<a hidden class="anchor" aria-hidden="true" href="#value-iteration">#</a></h3>
<p align="center">
 <img src = "images/value_iteration_psuedocode.png">
</p>
<h2 id="model-free-prediction">Model-Free Prediction<a hidden class="anchor" aria-hidden="true" href="#model-free-prediction">#</a></h2>
<p><strong>Model-free prediction</strong> algorithms aim to estimate the <strong>value function of a certain policy</strong> <em>without knowing the MDP</em>.</p>
<h3 id="monte-carlo-prediction">Monte-Carlo Prediction<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-prediction">#</a></h3>
<p align="center">
 <img src = "images/mc_prediction_pseudocode.png">
</p>
<h3 id="temporal-difference-learning">Temporal-Difference Learning<a hidden class="anchor" aria-hidden="true" href="#temporal-difference-learning">#</a></h3>
<p align="center">
 <img src = "images/td_learning_pseudocode.png">
</p>
<h3 id="difference-between-monte-carlo-mc-and-temporal-difference-td">Difference between Monte-Carlo (MC) and Temporal-Difference (TD)<a hidden class="anchor" aria-hidden="true" href="#difference-between-monte-carlo-mc-and-temporal-difference-td">#</a></h3>
<p align="center">
 <img src = "images/dp_and_mc.png">
</p>
<h2 id="model-free-control">Model-Free Control<a hidden class="anchor" aria-hidden="true" href="#model-free-control">#</a></h2>
<p>Model-free control algorithms can be divided into two groups: on-policy control and off-policy control.</p>
<ul>
<li>
<p><strong>On-policy control</strong>: Learn about policy $\pi$ from experience sampled from $\pi$</p>
</li>
<li>
<p><strong>Off-policy control</strong>: Learn about policy $\pi$ from experience sampled from policy $\mu \neq \pi$</p>
</li>
</ul>
<p>Both types of control algorithms are widely utilized in RL.</p>
<h3 id="on-policy-monte-carlo-control">On-policy Monte-Carlo Control<a hidden class="anchor" aria-hidden="true" href="#on-policy-monte-carlo-control">#</a></h3>
<p align="center">
 <img src = "images/on_policy_mc_pseudocode.png">
</p>
<h3 id="on-policy-temporal-difference-control">On-policy Temporal-Difference Control<a hidden class="anchor" aria-hidden="true" href="#on-policy-temporal-difference-control">#</a></h3>
<p align="center">
 <img src = "images/sarsa_psuedocode.png">
</p>
<h3 id="off-policy-temporal-difference-control">Off-policy Temporal-Difference Control<a hidden class="anchor" aria-hidden="true" href="#off-policy-temporal-difference-control">#</a></h3>
<p align="center">
 <img src = "images/q_learning_psuedocode.png">
</p>
<h2 id="policy-gradient-methods">Policy Gradient Methods<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-methods">#</a></h2>
<p>In policy gradient, policy parameter $\theta$ is updated by some scalar performance measure $\mathcal{J}(\theta)$ with respect to the policy parameter. To maximize policy performance, their updates approximate <strong>gradient ascent</strong> in $\mathcal{J}$:</p>
<p>$$\begin{align}
\theta_{t+1} = \theta_t + \alpha \nabla \mathcal{J}(\theta_t)
\end{align}$$</p>
<p>where $\alpha$ is a step-size parameter.</p>
<p>Then what can be the performance measure $\mathcal{J}(\theta)$ for an MDP policy in finite episodes?</p>
<p>In the episodic case, we have trajectory $\tau$. Then, we can define $\mathcal{J}(\theta)$ as its value.</p>
<p>$$\begin{align}
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d^\pi(s)v_\pi(s) = \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a|s) q_\pi(s, a)
\end{align}$$</p>
<p>where $d^\pi$ is the stationary distribution for the Markov chain for $\pi_\theta$.</p>
<p>Then the gradient of $\mathcal{J}$ can be reformatted as the following:</p>
<p>$$\begin{aligned}
\nabla_\theta \mathcal{J}(\theta) &amp;= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a|s) Q_\pi(s, a) &amp; \\
&amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q_\pi(s, a) \nabla_\theta \pi_\theta(a|s) \\
&amp;= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) Q_\pi(s, a) \dfrac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)} \\
&amp;= \mathbb{E_\pi} [Q_\pi(s,a) \nabla_\theta \log \pi_\theta(s,a)]
\end{aligned}$$</p>
<p>where $\mathbb{E_\pi}$ refers to $\mathbb{E_{s \sim d_\pi, a \sim \pi_\theta}}$ and $Q_\pi$ means the <strong>true state-value</strong> under policy $\pi$.</p>
<h3 id="policy-based-reinforcement-learning">Policy Based Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#policy-based-reinforcement-learning">#</a></h3>
<p>RL algorithms that directly generate the policy from experience: <strong>Policy-based RL</strong>.</p>
<h3 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-policy-gradient">#</a></h3>
<p>$$\begin{align}
\mathbb{E_\pi} [G_t|s_t, a_t] = Q_\pi (s_t, a_t)
\end{align}$$</p>
<p align="center">
 <img src = "images/reinforce_pseudocode.png">
</p>
<h3 id="actor-critic-policy-gradient">Actor-Critic Policy Gradient<a hidden class="anchor" aria-hidden="true" href="#actor-critic-policy-gradient">#</a></h3>
<p>Yet the algorithm <strong>REINFORCE</strong> has a disadvantage of high gradient variance.</p>
<p>Thus, to reduce variance, a <strong>critic</strong> can be used instead of <strong>return</strong> to estimate $Q_\pi$.</p>
<p>$$\begin{align}
q_\phi (s, a) = Q_\pi(s, a)
\end{align}$$</p>
<p>Therefore, the policy gradient is changed as</p>
<p>$$\begin{align}
\nabla_\theta \mathcal{J}(\theta) = \mathbb{E_\pi} [q_\phi(s,a) \nabla_\theta \log \pi_\theta(s,a)]\
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta (a_t|s_t) q_\phi(s_t, a_t)
\end{align}$$</p>
<p>Critic can be updated by</p>
<ul>
<li>Monte-Carlo evaluation</li>
<li>TD(0)</li>
<li>TD($\lambda$)</li>
</ul>
<p>For example, if we use TD(0), then in timestep $t$ the critic will be updated as</p>
<p>$$\begin{align}
\phi \leftarrow \phi + \beta (r_t + \gamma q_\phi(s_{t+1}, a_{t+1}) - q_\phi(s_t, a_t)) \nabla_\phi q_\phi(s_t, a_t)
\end{align}$$</p>
<p align="center">
 <img src = "images/actor_critic_pseudocode.png">
</p>
<h2 id="model-based-reinforcement-learning">Model-based Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#model-based-reinforcement-learning">#</a></h2>
<p><strong>Model-based RL</strong>: it learns a model from experience and plans value function and/or policy from the model.</p>
<h3 id="what-is-a-model">What is a Model?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-model">#</a></h3>
<p>A <em>Model</em> $\mathcal{M}$ is a representation of an MDP &lt;$\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}$&gt; parametrized by $\eta$. Assuming we know the state space $\mathcal{S}$ and action space $\mathcal{A}$, a model $&lt;\mathcal{P}, \mathcal{R}&gt;$ can represent transitions</p>
<p>$$\begin{align}
S_{t+1} \sim \mathcal{P_\eta(S_{t+1}|S_t, A_t)} \\
R_{t+1} \sim \mathcal{R_\eta(R_{t+1}|S_t, A_t)}
\end{align}$$</p>
<p>The model $&lt;\mathcal{P}, \mathcal{R}&gt;$ is learned from experience ${S_1, A_1, R_2, &hellip; , S_T}$.</p>
<p>A model can be parametrized in various ways: from lookup table models to deep neural networks.</p>
<p align="center">
 <img src = "images/dyna_pseudocode.png">
</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://moripiri.github.io/tags/rl/">RL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://moripiri.github.io/posts/vibe-coding-game/">
    <span class="title">« Prev</span>
    <br>
    <span>Vibe Coding으로 만드는 웹 게임</span>
  </a>
  <a class="next" href="https://moripiri.github.io/posts/my-first-post/">
    <span class="title">Next »</span>
    <br>
    <span>이거 어떻게 만든 거에요?</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://moripiri.github.io/">mori-blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
